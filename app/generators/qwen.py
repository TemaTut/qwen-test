from io import BytesIO
from pathlib import Path
from uuid import UUID
import torch
from PIL import Image
from diffusers import DiffusionPipeline


def generate_image(
    task_id: UUID,
    image_1: bytes,
    image_2: bytes | None = None,
    prompt: str = "",
    negative: str = "",
    num_inference_steps: int = 30,
):
    cache_path = Path(__file__).parent.parent.parent / "models"

    # üü¢ –ö–û–†–†–ï–ö–¢–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê ‚Äî –†–ê–ë–û–¢–ê–ï–¢ –ù–ê A6000 (48GB)
    pipe = DiffusionPipeline.from_pretrained(
        str(cache_path / "Qwen/Qwen-Image-Edit-2509"),
        torch_dtype=torch.float16,  # —ç–∫–æ–Ω–æ–º–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        device_map="auto",  # –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π offload
    )

    # memory-efficient attention (—ç–∫–æ–Ω–æ–º–∏—è VRAM)
    pipe.enable_xformers_memory_efficient_attention()

    # –ù–ï –ù–£–ñ–ù–û pipe.to("cuda") ‚Äî device_map="auto" —Å–∞–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç –º–æ–¥–µ–ª—å

    pipe.set_progress_bar_config(disable=None)

    # üñºÔ∏è preprocessing
    images = [Image.open(BytesIO(image_1)).convert("RGB")]
    if image_2:
        images.append(Image.open(BytesIO(image_2)).convert("RGB"))

    inputs = {
        "image": images,
        "prompt": prompt,
        "generator": torch.manual_seed(0),
        "negative_prompt": negative,
        "num_inference_steps": num_inference_steps,
        "num_images_per_prompt": 1,
    }

    # üî• –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (safe offload)
    with torch.inference_mode():
        output = pipe(**inputs)

        image = output.images[0]
        save_path = Path(__file__).parent.parent.parent / "output" / f"{task_id}.png"
        image.save(save_path)
